{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are working with the Sign Language MNIST dataset.\n",
    "First we need to load the dataset for training and testing from the CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "training_df = pd.read_csv(\"C:\\Machine Learning\\CSCI4050U_FinalProject\\dataset\\sign_mnist_train\\sign_mnist_train.csv\") # Need to find a way to provide a relative path\n",
    "testing_df = pd.read_csv(\"C:\\Machine Learning\\CSCI4050U_FinalProject\\dataset\\sign_mnist_test\\sign_mnist_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract Labels and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = training_df['label']\n",
    "training_df.drop(['label'], axis=1, inplace=True)\n",
    "\n",
    "y_test = testing_df['label']\n",
    "testing_df.drop(['label'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Constrants and reshape the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust parameters as needed\n",
    "size = 28\n",
    "channels = 1\n",
    "batch = 128\n",
    "epochs = 100\n",
    "\n",
    "X_train = training_df.values.reshape(training_df.shape[0], size, size, channels)\n",
    "X_test = testing_df.values.reshape(testing_df.shape[0], size, size, channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Augmentation and Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Ting\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Link to instructions for getting tensorflow working\n",
    "# https://www.tensorflow.org/install/pip#windows-native_1\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    zoom_range=0.2,\n",
    "    width_shift_range=.2,\n",
    "    height_shift_range=.2,\n",
    "    rotation_range=15,\n",
    "    brightness_range=[0.8, 1.2],\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1./255\n",
    ")\n",
    "\n",
    "X_train = train_datagen.flow(X_train, y_train, batch_size=batch)\n",
    "X_test = test_datagen.flow(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from lightning.pytorch import LightningModule\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torchmetrics\n",
    "\n",
    "class BaseModel(LightningModule):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.accuracy = torchmetrics.classification.Accuracy(\n",
    "            task=\"multiclass\",\n",
    "            num_classes=num_classes)\n",
    "        self.model = self.build_model()\n",
    "        \n",
    "    def build_model(self):\n",
    "        raise Exception(\"Not yet implemented\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def loss(self, logits, target):\n",
    "        return nn.functional.cross_entropy(logits, target)\n",
    "    \n",
    "    def shared_step(self, mode:str, batch:Tuple[Tensor, Tensor], batch_index:int):\n",
    "        x, target = batch\n",
    "        output = self.forward(x)\n",
    "        loss = self.loss(output, target)\n",
    "        self.accuracy(output, target)\n",
    "        self.log(f\"{mode}_step_acc\", self.accuracy, prog_bar=True)\n",
    "        self.log(f\"{mode}_step_loss\", loss, prog_bar=False)\n",
    "        return loss\n",
    "    \n",
    "    def training_step(self, batch, batch_index):\n",
    "        return self.shared_step('train', batch, batch_index)\n",
    "    \n",
    "    def validation_step(self, batch, batch_index):\n",
    "        return self.shared_step('val', batch, batch_index)\n",
    "    \n",
    "    def test_step(self, batch, batch_index):\n",
    "        return self.shared_step('test', batch, batch_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Functions from Assignment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchinfo\n",
    "def describe(model, **kwargs):\n",
    "    return torchinfo.summary(model,\n",
    "                             input_size=(batch_size, 1, 28, 28),\n",
    "                             col_names=['input_size', 'output_size', 'kernel_size', 'num_params'],\n",
    "                             row_settings=['ascii_only'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "from lightning.pytorch import Trainer, seed_everything\n",
    "\n",
    "def train(model):\n",
    "    name = model.__class__.__name__\n",
    "    shutil.rmtree(f'./lightning_logs/{name}', ignore_errors=True)\n",
    "    seed_everything(0, workers=True)\n",
    "    logger = CSVLogger('./lightning_logs', name=name)\n",
    "    trainer = Trainer(max_epochs=3, logger=logger, deterministic=True)\n",
    "    trainer.fit(model,\n",
    "                train_dataloaders=training_dataloader,\n",
    "                val_dataloaders=val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_metrics(name):\n",
    "    df = pd.read_csv(f'./lightning_logs/{name}/version_0/metrics.csv')\n",
    "    df.set_index('step', inplace=True)\n",
    "    ax = df[['train_step_acc']].dropna().plot()\n",
    "    df[['val_step_acc']].dropna().plot(ax=ax)\n",
    "    return df[['val_step_acc']].dropna().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolution Net Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(BaseModel):\n",
    "    def __init__(self, num_classes, num_kernels, kernel_size, pool_size):\n",
    "        self.num_kernels = num_kernels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.pool_size = pool_size\n",
    "        super().__init__(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
